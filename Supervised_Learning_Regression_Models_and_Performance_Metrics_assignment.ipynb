{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1 : What is Simple Linear Regression (SLR)? Explain its purpose?\n",
        "\n",
        "Answer 1 : Simple Linear Regression (SLR) is a fundamental statistical method used to model the relationship between two variables by fitting a straight line to the observed data. It allows you to estimate how one variable changes when the other variable changes.\n",
        "\n",
        "In SLR, there are always exactly two variables involved:\n",
        "1. Independent Variable (X): The predictor or explanatory variable (the input).\n",
        "2. Dependent Variable (Y): The response or target variable (the output you want to predict).\n",
        "\n",
        "The Mathematical Equation : The relationship is expressed using the following linear equation:\n",
        "\n",
        "Y=β0​+β1​X+ϵ\n",
        "\n",
        "Where:\n",
        " - Y: The predicted value of the dependent variable.\n",
        " - X: The value of the independent variable.\n",
        " - β0​ (Intercept): The value of Y when X is 0.\n",
        " - β1​ (Slope): The rate at which Y changes for every one-unit increase in X.\n",
        " - ϵ (Error Term): The difference between the observed values and the predicted values (residuals).\n",
        "\n",
        "The goal of SLR is to find the \"Line of Best Fit\" that minimizes the sum of the squared errors (residuals) between the actual data points and the line. This method is often called Ordinary Least Squares (OLS).\n",
        "\n",
        "The Purpose of SLR : Simple Linear Regression is primarily used for two purposes:\n",
        "1. Prediction (Forecasting) To predict the value of a dependent variable based on a known independent variable.\n",
        "\n",
        "Example: Predicting a student's exam score (Y) based on the number of hours they studied (X).\n",
        "\n",
        "2. Inference (Relationship Analysis) To understand the nature and strength of the relationship between variables.\n",
        "\n",
        "Example: Determining if there is a significant positive relationship between advertising spend (X) and sales revenue (Y). It helps answer questions like, \"Does increasing advertising actually lead to higher sales?\n",
        "\n",
        "Question 2: What are the key assumptions of Simple Linear Regression?\n",
        "\n",
        "Answer 2 : To ensure the results of a Simple Linear Regression model are valid and trustworthy, the data must satisfy four main assumptions. These are often remembered by the acronym LINE .\n",
        "\n",
        "1. Linearity : The relationship between the independent variable (X) and the dependent variable (Y) must be linear.\n",
        " - What it means: The change in Y due to a one-unit change in X is constant. If you plot the data, it should look roughly like a straight line, not a curve.\n",
        " - How to check: Look at a scatter plot of X vs. Y. If the points follow a U-shape or curve, this assumption is violated.\n",
        "\n",
        "2. Independence : The observations in the dataset must be independent of one another.\n",
        " - What it means: The value of one observation should not influence or predict the next. This is critical in time-series data where \"autocorrelation\" might occur (e.g., stock prices today depend on stock prices yesterday).\n",
        " - How to check: Usually determined by the study design. For time-series data, the Durbin-Watson test is used.\n",
        "\n",
        "3. Normality (of Residuals) : The residuals (errors) of the model should follow a normal (bell-shaped) distribution.\n",
        "\n",
        " - What it means: Most prediction errors should be close to zero, with fewer errors as you get further away. The model shouldn't be systematically over-predicting or under-predicting in a skewed way.\n",
        "\n",
        " - How to check: Use a Q-Q plot (Quantile-Quantile plot) or a histogram of the residuals.\n",
        "\n",
        "4. Equal Variance (Homoscedasticity) : The variance of the residuals should be constant across all values of X.\n",
        "\n",
        " - What it means: The \"spread\" or \"scatter\" of the data points around the regression line should be roughly the same from the beginning of the line to the end. The opposite is Heteroscedasticity (different spread), which often looks like a \"cone\" or \"fan\" shape where errors get larger as X increases.\n",
        "\n",
        "Question 3: Write the mathematical equation for a simple linear regression model and explain each term ?\n",
        "\n",
        "Answer 3 :\n",
        "The Simple Linear Regression Equation :\n",
        "The mathematical model for Simple Linear Regression is generally expressed in two forms: the theoretical population model (which includes the error term) and the estimated model (which is used for making actual predictions).\n",
        "\n",
        "1. The Population Regression Equation : This equation represents the \"true\" relationship in the population, acknowledging that data points rarely fall perfectly on a straight line.\n",
        "\n",
        "Y=β0​+β1​X+ϵ\n",
        "\n",
        "Here is the breakdown of each term:\n",
        "\n",
        " - Y (Dependent Variable): The variable you are trying to predict or explain (also called the Target or Response variable).\n",
        "\n",
        "   - Example: Salary.\n",
        "\n",
        " - X (Independent Variable): The variable used to make the prediction (also called the Predictor or Feature).\n",
        "\n",
        "   - Example: Years of Experience.\n",
        "\n",
        " - β0​ (The Intercept): The expected value of Y when X is zero. It determines where the line crosses the Y-axis.\n",
        "\n",
        "   - Interpretation: If \"Years of Experience\" is 0, β0​ is the starting base salary.\n",
        "\n",
        " - β1​ (The Slope): This indicates the change in Y for every one-unit increase in X. It determines the steepness and direction of the line.\n",
        "\n",
        "   - Interpretation: For every 1 additional year of experience, salary increases by β1​.\n",
        "\n",
        " - ϵ (The Error Term / Residual): This represents the \"noise\" or random variation in the data that the model cannot explain. It is the difference between the actual observed value and the value predicted by the line.\n",
        "\n",
        "   - Note: In the estimated equation used for prediction, this term disappears because the regression line represents the average expected value.\n",
        "\n",
        "2. The Estimated Regression Equation : When we actually run a regression (like using Python's scikit-learn), we obtain the \"Line of Best Fit.\" The equation changes slightly to denote predicted values:\n",
        "\n",
        "Y^=b0​+b1​X\n",
        "\n",
        " - Y^ (\"Y-hat\"): The predicted value of Y.\n",
        "\n",
        " - b0​ and b1​: These are the specific numerical estimates calculated from your data sample to estimate the unknown population parameters β0​ and β1​.\n",
        "\n",
        "Question 4: Provide a real-world example where simple linear regression can be applied ?\n",
        "\n",
        "Answer 4 : Real-World Example: Food Delivery Time :-\n",
        "\n",
        "Considering you are a foodie, let's look at a scenario you might encounter often: ordering food online.\n",
        "\n",
        "Scenario: A food delivery app wants to predict how long it will take to deliver an order based on how far the customer's house is from the restaurant.\n",
        "\n",
        " - Independent Variable (X): Distance (in kilometers).\n",
        "\n",
        " - Dependent Variable (Y): Delivery Time (in minutes).\n",
        "\n",
        "1. The Relationship :- We expect a positive linear relationship: as the distance (X) increases, the delivery time (Y) also increases.\n",
        "\n",
        "2. The Equation : Let's say we analyze the data from past orders and our regression model gives us this equation:\n",
        "\n",
        "Y=10+5X\n",
        "\n",
        "Here is what the terms mean in this real-world context:\n",
        "\n",
        " - Intercept (β0​=10): This is the baseline time when the distance is 0 km. In reality, this represents the fixed preparation and packaging time (10 minutes) that happens regardless of how far the driver has to travel.\n",
        "\n",
        " - Slope (β1​=5): This is the rate of change. It means for every additional 1 kilometer of distance, the delivery time increases by 5 minutes (due to traffic, speed limits, etc.).\n",
        "\n",
        "3. Making a Prediction : If you order from a restaurant that is 4 km away (X=4), you can predict the arrival time:\n",
        "\n",
        "Y=10+5(4)\n",
        "\n",
        "Y=10+20\n",
        "\n",
        "Y=30 minutes\n",
        "\n",
        "Question 5 : What is the method of least squares in linear regression?\n",
        "\n",
        "Answer 5 : The Method of Least Squares (often called Ordinary Least Squares or OLS) is the technique used to determine the best-fitting line through your data points.\n",
        "\n",
        "In simple terms, \"best-fitting\" means finding the line that is as close as possible to all the data points at once. To do this, the method minimizes the total \"error\" between the actual data and the predicted line.\n",
        "\n",
        "How It Works :\n",
        "1. The Residual (The Error) : For every data point, there is a difference between the actual value (Y) and the predicted value (Y^) on the line. This difference is called a residual.\n",
        "\n",
        "Residual=Yactual​−Ypredicted​\n",
        " - Positive Residual: The data point is above the line.\n",
        " - Negative Residual: The data point is below the line.\n",
        "\n",
        "2. The Problem with Summing Errors : If you simply added up all the residuals, the positive and negative values would cancel each other out, likely giving you a sum near zero even if the line fits poorly.\n",
        "\n",
        "3. The Solution: Squaring the Errors : To solve this, the Method of Least Squares takes each residual and squares it.\n",
        " - Squaring makes all values positive (so they don't cancel out).\n",
        " - Squaring penalizes large errors more heavily than small ones (e.g., an error of 2 becomes 4, but an error of 10 becomes 100).\n",
        "\n",
        "4. The Goal: Minimize the Sum : The algorithm adjusts the Slope (m) and Intercept (b) of the line until the Sum of Squared Errors (SSE) is as small as mathematically possible.\n",
        "\n",
        "Minimize ∑(Yi​−Y^i​)2\n",
        "\n",
        "Question 6: What is Logistic Regression? How does it differ from Linear Regression?\n",
        "\n",
        "Answer 6 : Despite its confusing name, Logistic Regression is actually a classification algorithm, not a regression algorithm. It is used to predict a discrete outcome (categories), usually binary (1 or 0, Yes or No, True or False).\n",
        "\n",
        "Instead of predicting an exact numerical value (like \"price\" or \"temperature\"), it predicts the probability that a given input belongs to a specific class.\n",
        "\n",
        "The Mechanism: It fits data to an \"S\" shaped curve called the Sigmoid Function (or Logistic Function). This function takes any input value (from −∞ to +∞) and \"squeezes\" it to a value strictly between 0 and 1.\n",
        "\n",
        "The probability equation looks like this:\n",
        "\n",
        "P(Y=1)=1+e−(β0​+β1​X)1​\n",
        "\n",
        "If the calculated probability is greater than 0.5 (50%), the model usually classifies it as \"1\" (Yes); otherwise, it is \"0\" (No).\n",
        "\n",
        "How it differs from Linear Regression :\n",
        " - Type of problem :\n",
        "   - Linear Regression is used for regression tasks where the output is continuous (e.g., price, temperature).\n",
        "   - Logistic Regression is used for classification tasks where the output is categorical, usually binary (0/1, yes/no).\n",
        " - Output and function :\n",
        "   - Linear Regression predicts a numeric value directly as a linear combination of inputs, which can take any real value (−∞,∞).\n",
        "   - Logistic Regression predicts a probability using the logistic (sigmoid) function, producing values only between 0 and 1, then applies a threshold to assign a class label.\n",
        " - Modeling and loss :\n",
        "   - Linear Regression typically uses Ordinary Least Squares (minimizing squared error) to estimate parameters.\n",
        "   - Logistic Regression uses Maximum Likelihood Estimation and minimizes logistic (log) loss, which is appropriate for probability modeling and classification.\n",
        " - Use in practice :\n",
        "   - Linear Regression is preferred when the target is continuous and the goal is numeric prediction.\n",
        "   - Logistic Regression is preferred when the goal is to classify observations and estimate the probability of belonging to a class, with performance evaluated via accuracy, precision, recall, and F1-score instead of MSE or RMSE.\n",
        "\n",
        "Question 7: Name and briefly describe three common evaluation metrics for regression models?\n",
        "\n",
        "Answer 7 : Three Common Evaluation Metrics for Regression Models :\n",
        "To determine how \"good\" a regression model is, we need to measure how far its predictions (Y^) are from the actual values (Y). Here are the three most common metrics used to evaluate performance:\n",
        "\n",
        "1. Mean Absolute Error (MAE) : This is the simplest metric. It calculates the average of the absolute differences between predicted and actual values.\n",
        " - Formula : MAE=n1​∑∣Y−Y^∣\n",
        " - Interpretation : It tells you, on average, how wrong your predictions are in the actual units of the data. It treats all errors equally (no heavy penalty for huge errors).\n",
        " - Example: If predicting house prices, an MAE of $5,000 means your predictions are typically off by $5,000.\n",
        "\n",
        "2. Mean Squared Error (MSE) : This metric squares the difference between predicted and actual values before averaging them.\n",
        " - Formula : MSE=n1​∑(Y−Y^)2\n",
        " - Interpretation: By squaring the errors, MSE heavily penalizes large errors. It is useful when you want to ensure your model doesn't make any massive blunders. However, the result is in \"squared units\" (e.g., dollars squared), which makes it difficult to interpret directly.\n",
        "\n",
        "3. Root Mean Squared Error (RMSE) : This is simply the square root of the MSE. It brings the error unit back to the original unit of the data (like dollars or degrees).\n",
        "\n",
        " - Formula : RMSE=MSE​=n1​∑(Y−Y^)2​\n",
        " - Interpretation : It acts as the standard deviation of the prediction errors. Like MSE, it penalizes large errors more than MAE, but it is easier to read because the unit matches your target variable. It is one of the most popular metrics for regression.\n",
        "\n",
        "Question 8: What is the purpose of the R-squared metric in regression analysis?\n",
        "\n",
        "Answer 8 : R-squared (also called the Coefficient of Determination) is a statistical measure that represents the \"goodness of fit\" of a regression model.\n",
        "\n",
        "While metrics like RMSE (Question 7) tell you \"how wrong\" the predictions are (in dollars, degrees, etc.), R-squared tells you \"how much of the pattern\" your model has managed to capture.\n",
        "\n",
        "It is expressed as a value between 0 and 1 (or 0% to 100%).\n",
        "\n",
        "The Core Purpose: Explaining Variance : The primary purpose of R-squared is to answer the question:\n",
        "\n",
        "\"Of all the variation seen in the data, what percentage can be explained by my model?\"\n",
        " - R2=1 (100%): The model explains all the variability in the response data around its mean. The data points fall perfectly on the regression line.\n",
        " - R2=0 (0%): The model explains none of the variability. The model is no better than simply guessing the average (Mean) for every prediction.\n",
        "\n",
        "How it is Calculated : R-squared compares your regression model to a \"baseline\" model. The baseline model is just a horizontal line drawn at the average (Mean) of the data.\n",
        "\n",
        "R2=1−Total VariationUnexplained Variation​\n",
        "\n",
        "Mathematically:\n",
        "\n",
        "R2=1−SStot​SSres​​\n",
        "\n",
        " - SSres​ (Sum of Squared Residuals): The error of your model.\n",
        "\n",
        " - SStot​ (Total Sum of Squares): The error of the baseline (mean) model.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lTx97pojzQca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9: Write Python code to fit a simple linear regression model using scikit-learn and print the slope and intercept ?\n",
        "# Answer 9 :\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# 1. Prepare the data\n",
        "# X (Independent variable) must be a 2D array (list of lists)\n",
        "X = np.array([[1], [2], [3], [4], [5]])\n",
        "\n",
        "# y (Dependent variable) is a 1D array\n",
        "y = np.array([2, 4, 5, 4, 5])\n",
        "\n",
        "# 2. Create an instance of the model\n",
        "model = LinearRegression()\n",
        "\n",
        "# 3. Fit the model (Train the algorithm)\n",
        "model.fit(X, y)\n",
        "\n",
        "# 4. Retrieve the Slope (m) and Intercept (b)\n",
        "# coef_ returns an array, so we select the first element [0]\n",
        "slope = model.coef_[0]\n",
        "intercept = model.intercept_\n",
        "\n",
        "# 5. Print the results\n",
        "print(f\"Slope (Coefficient): {slope}\")\n",
        "print(f\"Intercept: {intercept}\")\n",
        "\n",
        "# Optional: Predict a value for X = 6\n",
        "prediction = model.predict([[6]])\n",
        "print(f\"Prediction for X=6: {prediction[0]}\")\n",
        "\n",
        "# Explanation of Key Lines :\n",
        "# 1. X = ...: Scikit-learn expects the independent variable (X) to be a 2D array (e.g., [[1], [2]] instead of [1, 2]), even if there is only one feature. This is because the library is designed to handle multiple variables by default.\n",
        "\n",
        "# 2. model.fit(X, y): This is the step where the \"Method of Least Squares\" actually happens. The model calculates the best line through the data.\n",
        "\n",
        "# 3. model.coef_: This holds the estimated slope (β1​).\n",
        "\n",
        "# 4. model.intercept_: This holds the estimated y-intercept (β0​)."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRx_3GI7G4XK",
        "outputId": "1a9318ce-0eb6-4bea-f666-85180ace7062"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Slope (Coefficient): 0.6\n",
            "Intercept: 2.2\n",
            "Prediction for X=6: 5.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: How do you interpret the coefficients in a simple linear regression model?\n",
        "\n",
        "Answer 10 : Interpreting the coefficients is the most critical part of reporting your analysis. It translates the raw math back into a real-world story.\n",
        "\n",
        "In Simple Linear Regression (Y=β0​+β1​X), there are two coefficients to interpret:\n",
        "1. The Slope (β1​) : The slope tells you how much the dependent variable changes for a specific change in the independent variable.\n",
        " - Interpretation Template: \"For every one-unit increase in X, we expect Y to increase/decrease by [Slope Value].\"\n",
        " - Sign Matters:\n",
        "   - Positive Slope (+): As X goes up, Y goes up.\n",
        "   - Negative Slope (-): As X goes up, Y goes down.\n",
        "\n",
        "2. The Intercept (β0​) : The intercept tells you the starting point of the model.\n",
        " - Interpretation Template: \"When X is zero, the expected value of Y is [Intercept Value].\"\n",
        " - The \"Zero\" Caveat: The intercept only makes sense if X=0 is actually possible in reality. If X can never be zero (e.g., \"Human Height\"), the intercept is just a mathematical anchor with no physical meaning."
      ],
      "metadata": {
        "id": "8JDNlCkzIL0K"
      }
    }
  ]
}